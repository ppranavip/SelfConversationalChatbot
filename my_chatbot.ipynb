{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7AZft0_fIOU",
        "outputId": "47d00258-6ab3-400e-c3d4-31aed2ff2b90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for trax (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q -U git+https://github.com/google/trax@master\n",
        "import json\n",
        "import random\n",
        "import numpy as np\n",
        "from termcolor import colored\n",
        "import trax\n",
        "from trax import layers as tl\n",
        "from trax.supervised import training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5AyJ2RqdgTFn",
        "outputId": "c9f2afee-68eb-445c-a67f-a213f5e5a018"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2czPGB4wgdQ6",
        "outputId": "0648d27f-a490-4e49-b354-0b366bfb7f06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-LQcSRPfgiz2",
        "outputId": "b2beec8a-e28d-4625-9f31-7fb43bbaedd4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "drive  sample_data\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxQii3G7fJPG"
      },
      "outputs": [],
      "source": [
        "# filename of the MultiWOZ dialogue dataset\n",
        "DATA_FILE = 'data.json'\n",
        "\n",
        "# data directory\n",
        "DATA_DIR = '/content/drive/MyDrive/data/MultiWOZ_2.1'\n",
        "\n",
        "# dictionary where we will load the dialogue dataset\n",
        "DIALOGUE_DB = {}\n",
        "\n",
        "# vocabulary filename\n",
        "VOCAB_FILE = 'en_32k.subword'\n",
        "\n",
        "# vocabulary file directory\n",
        "VOCAB_DIR = '/content/drive/MyDrive/data/vocabs'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-70MphogjqV"
      },
      "outputs": [],
      "source": [
        "# help function to load a JSON file\n",
        "def load_json(directory, file):\n",
        "    with open(f'{directory}/{file}') as file:\n",
        "        db = json.load(file)\n",
        "    return db\n",
        "\n",
        "# load the dialogue data set into our dictionary\n",
        "DIALOGUE_DB = load_json(DATA_DIR, DATA_FILE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HMBnqu6kiIN_",
        "outputId": "36832cd2-7471-40aa-f74a-9f75d88c2f3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The number of dialogues is: 10438\n"
          ]
        }
      ],
      "source": [
        "print(f'The number of dialogues is: {len(DIALOGUE_DB)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89wssMBDi70I",
        "outputId": "190fa7eb-31a1-46fc-9e2c-c0c7be1b7115"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['SNG01856.json', 'SNG0129.json', 'PMUL1635.json', 'MUL2168.json', 'SNG0073.json', 'SNG01445.json', 'MUL2105.json']\n"
          ]
        }
      ],
      "source": [
        "# print 7 keys from the dataset to see the filenames\n",
        "print(list(DIALOGUE_DB.keys())[0:7])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-R0s1OFsi-_3",
        "outputId": "f170fe20-8a50-44ad-b5c5-22540e588dea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_keys(['goal', 'log'])\n"
          ]
        }
      ],
      "source": [
        "# get keys of the fifth file in the list above\n",
        "print(DIALOGUE_DB['SNG0073.json'].keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UNT8J_-qjBZr",
        "outputId": "3193da6e-bcca-415f-ef03-4a6a513bf03c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'taxi': {'info': {'leaveAt': '17:15',\n",
              "   'destination': 'pizza hut fen ditton',\n",
              "   'departure': \"saint john's college\"},\n",
              "  'reqt': ['car type', 'phone'],\n",
              "  'fail_info': {}},\n",
              " 'police': {},\n",
              " 'hospital': {},\n",
              " 'hotel': {},\n",
              " 'attraction': {},\n",
              " 'train': {},\n",
              " 'message': [\"You want to book a <span class='emphasis'>taxi</span>. The taxi should go to <span class='emphasis'>pizza hut fen ditton</span> and should depart from <span class='emphasis'>saint john's college</span>\",\n",
              "  \"The taxi should <span class='emphasis'>leave after 17:15</span>\",\n",
              "  \"Make sure you get <span class='emphasis'>car type</span> and <span class='emphasis'>contact number</span>\"],\n",
              " 'restaurant': {}}"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "DIALOGUE_DB['SNG0073.json']['goal']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "izsZdDKOjD7u",
        "outputId": "7aefadc1-a832-4ec6-ac20-997765a11040"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'text': \"I would like a taxi from Saint John's college to Pizza Hut Fen Ditton.\",\n",
              " 'metadata': {},\n",
              " 'dialog_act': {'Taxi-Inform': [['Dest', 'pizza hut fen ditton'],\n",
              "   ['Depart', \"saint john 's college\"]]},\n",
              " 'span_info': [['Taxi-Inform', 'Dest', 'pizza hut fen ditton', 11, 14],\n",
              "  ['Taxi-Inform', 'Depart', \"saint john 's college\", 6, 9]]}"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# get first element of the log list\n",
        "DIALOGUE_DB['SNG0073.json']['log'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2mu1rma9jGOC",
        "outputId": "c9559fa5-0aa0-4022-d1de-20b291cac93b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Person 1:  I would like a taxi from Saint John's college to Pizza Hut Fen Ditton.\n",
            " Person 2:  What time do you want to leave and what time do you want to arrive by?\n"
          ]
        }
      ],
      "source": [
        "print(' Person 1: ', DIALOGUE_DB['SNG0073.json']['log'][0]['text'])\n",
        "print(' Person 2: ',DIALOGUE_DB['SNG0073.json']['log'][1]['text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5rPPXBlOjIRz"
      },
      "outputs": [],
      "source": [
        "# UNQ_C1\n",
        "# GRADED FUNCTION: get_conversation\n",
        "def get_conversation(file, data_db):\n",
        "    '''\n",
        "    Args:\n",
        "        file (string): filename of the dialogue file saved as json\n",
        "        data_db (dict): dialogue database\n",
        "\n",
        "    Returns:\n",
        "        string: A string containing the 'text' fields of  data[file]['log'][x]\n",
        "    '''\n",
        "\n",
        "    # initialize empty string\n",
        "    result = ''\n",
        "\n",
        "    # get length of file's log list\n",
        "    len_msg_log = len(data_db[file]['log'])\n",
        "\n",
        "    # set the delimiter strings\n",
        "    delimiter_1 = ' Person 1: '\n",
        "    delimiter_2 = ' Person 2: '\n",
        "\n",
        "    # loop over the file's log list\n",
        "    for i in range(len_msg_log):\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "\n",
        "        # get i'th element of file log list\n",
        "        cur_log = data_db[file]['log'][i]\n",
        "\n",
        "        # check if i is even\n",
        "        if i%2==0:\n",
        "            # append the 1st delimiter string\n",
        "            result += delimiter_1\n",
        "        else:\n",
        "            # append the 2nd delimiter string\n",
        "            result += delimiter_2\n",
        "\n",
        "        # append the message text from the log\n",
        "        result += cur_log['text']\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ws-q9tasjMoq",
        "outputId": "59ab8a8a-5d1b-464a-f80b-7ad466ea3ac9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Person 1: am looking for a place to to stay that has cheap price range it should be in a type of hotel Person 2: Okay, do you have a specific area you want to stay in? Person 1: no, i just need to make sure it's cheap. oh, and i need parking Person 2: I found 1 cheap hotel for you that includes parking. Do you like me to book it? Person 1: Yes, please. 6 people 3 nights starting on tuesday. Person 2: I am sorry but I wasn't able to book that for you for Tuesday. Is there another day you would like to stay or perhaps a shorter stay? Person 1: how about only 2 nights. Person 2: Booking was successful.\n",
            "Reference number is : 7GAWK763. Anything else I can do for you? Person 1: No, that will be all. Good bye. Person 2: Thank you for using our services.\n"
          ]
        }
      ],
      "source": [
        "file = 'SNG01856.json'\n",
        "conversation = get_conversation(file, DIALOGUE_DB)\n",
        "\n",
        "# print raw output\n",
        "print(conversation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88a2QnwejPKg",
        "outputId": "b5f5dff7-f664-4769-cf8f-9f9924f94e92"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Person 1: am looking for a place to to stay that has cheap price range it should be in a type of hotel \n",
            "Person 2: Okay, do you have a specific area you want to stay in? \n",
            "Person 1: no, i just need to make sure it's cheap. oh, and i need parking \n",
            "Person 2: I found 1 cheap hotel for you that includes parking. Do you like me to book it? \n",
            "Person 1: Yes, please. 6 people 3 nights starting on tuesday. \n",
            "Person 2: I am sorry but I wasn't able to book that for you for Tuesday. Is there another day you would like to stay or perhaps a shorter stay? \n",
            "Person 1: how about only 2 nights. \n",
            "Person 2: Booking was successful.\n",
            "Reference number is : 7GAWK763. Anything else I can do for you? \n",
            "Person 1: No, that will be all. Good bye. \n",
            "Person 2: Thank you for using our services.\n"
          ]
        }
      ],
      "source": [
        "def print_conversation(conversation):\n",
        "\n",
        "    delimiter_1 = 'Person 1: '\n",
        "    delimiter_2 = 'Person 2: '\n",
        "\n",
        "    split_list_d1 = conversation.split(delimiter_1)\n",
        "\n",
        "    for sublist in split_list_d1[1:]:\n",
        "        split_list_d2 = sublist.split(delimiter_2)\n",
        "        print(colored(f'Person 1: {split_list_d2[0]}', 'red'))\n",
        "\n",
        "        if len(split_list_d2) > 1:\n",
        "            print(colored(f'Person 2: {split_list_d2[1]}', 'green'))\n",
        "\n",
        "\n",
        "print_conversation(conversation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rmAgQ8MjXPo",
        "outputId": "187f809a-beb6-4a15-cf85-d73780d392ae"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'text': 'am looking for a place to to stay that has cheap price range it should be in a type of hotel',\n",
              " 'metadata': {},\n",
              " 'dialog_act': {'Hotel-Inform': [['Type', 'hotel'], ['Price', 'cheap']]},\n",
              " 'span_info': [['Hotel-Inform', 'Type', 'hotel', 20, 20],\n",
              "  ['Hotel-Inform', 'Price', 'cheap', 10, 10]]}"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "DIALOGUE_DB['SNG01856.json']['log'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2HhjRfqjatO",
        "outputId": "29bd4316-daf0-4f6d-bd75-90c6a5a10c8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'address': 'pool way, whitehill road, off newmarket road', 'area': 'east', 'entrance fee': '?', 'id': '1', 'location': [52.208789, 0.154883], 'name': 'abbey pool and astroturf pitch', 'openhours': '?', 'phone': '01223902088', 'postcode': 'cb58nt', 'pricerange': '?', 'type': 'swimmingpool'}\n"
          ]
        }
      ],
      "source": [
        "# this is an example of the attractions file\n",
        "attraction_file = open('/content/drive/MyDrive/data/MultiWOZ_2.1/attraction_db.json')\n",
        "attractions = json.load(attraction_file)\n",
        "print(attractions[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EMelxNK3jc8i",
        "outputId": "9db0a38e-aa7b-48d7-a30a-92b1c602ac13"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'department': 'neurosciences critical care unit', 'id': 0, 'phone': '01223216297'}\n"
          ]
        }
      ],
      "source": [
        "# this is an example of the hospital file\n",
        "hospital_file = open('/content/drive/MyDrive/data/MultiWOZ_2.1/hospital_db.json')\n",
        "hospitals = json.load(hospital_file)\n",
        "print(hospitals[0]) # feel free to index into other indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pznQoiJjn-h",
        "outputId": "34ccd36f-b38e-4778-c5ac-5cbb17a67887"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'address': '124 tenison road', 'area': 'east', 'internet': 'yes', 'parking': 'no', 'id': '0', 'location': [52.1963733, 0.1987426], 'name': 'a and b guest house', 'phone': '01223315702', 'postcode': 'cb12dp', 'price': {'double': '70', 'family': '90', 'single': '50'}, 'pricerange': 'moderate', 'stars': '4', 'takesbookings': 'yes', 'type': 'guesthouse'}\n"
          ]
        }
      ],
      "source": [
        "# this is an example of the hotel file\n",
        "hotel_file = open('/content/drive/MyDrive/data/MultiWOZ_2.1/hotel_db.json')\n",
        "hotels = json.load(hotel_file)\n",
        "print(hotels[0]) # feel free to index into other indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvkQhktGjrtI",
        "outputId": "56464dc4-a2d7-4d22-873a-6a8da006c330"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'name': 'Parkside Police Station', 'address': 'Parkside, Cambridge', 'id': 0, 'phone': '01223358966'}\n"
          ]
        }
      ],
      "source": [
        "# this is an example of the police file\n",
        "police_file = open('/content/drive/MyDrive/data/MultiWOZ_2.1/police_db.json')\n",
        "police = json.load(police_file)\n",
        "print(police[0]) # feel free to index into other indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTgQn6KijrwB",
        "outputId": "f8eb7d66-af0c-4ca2-8cc7-c171da38583c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'address': 'Regent Street City Centre', 'area': 'centre', 'food': 'italian', 'id': '19210', 'introduction': 'Pizza hut is a large chain with restaurants nationwide offering convenience pizzas pasta and salads to eat in or take away', 'location': [52.20103, 0.126023], 'name': 'pizza hut city centre', 'phone': '01223323737', 'postcode': 'cb21ab', 'pricerange': 'cheap', 'type': 'restaurant'}\n"
          ]
        }
      ],
      "source": [
        "# this is an example of a restaurant file\n",
        "restaurant_file = open('/content/drive/MyDrive/data/MultiWOZ_2.1/restaurant_db.json')\n",
        "restaurants = json.load(restaurant_file)\n",
        "print(restaurants[0]) # feel free to index into other indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--VyBZdgjry5",
        "outputId": "e951aad2-5d30-428a-a010-77bd69f3f095"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#####################################################\n",
            "#####################################################\n",
            "#  Copyright Cambridge Dialogue Systems Group, 2018 #\n",
            "#####################################################\n",
            "#####################################################\n",
            "\n",
            "Dataset contains the following files:\n",
            "1. data.json: the woz dialogue dataset, which contains the conversation  users and wizards, as well as a set of coarse labels for each user turn. This file contains both system and user dialogue acts annotated at the turn level. Files with multi-domain dialogues have \"MUL\" in their names. Single domain dialogues have either \"SNG\" or \"WOZ\" in their names.\n",
            "2. restaurant_db.json: the Cambridge restaurant database file, containing restaurants in the Cambridge UK area and a set of attributes.\n",
            "3. attraction_db.json: the Cambridge attraction database file, contining attractions in the Cambridge UK area and a set of attributes.\n",
            "4. hotel_db.json: the Cambridge hotel database file, containing hotels in the Cambridge UK area and a set of attributes.\n",
            "5. train_db.json: the Cambridge train (with artificial connections) database file, containing trains in the Cambridge UK area and a set of attributes.\n",
            "6. hospital_db.json: the Cambridge hospital database file, contatining information about departments.\n",
            "7. police_db.json: the Cambridge police station information.\n",
            "8. taxi_db.json: slot-value list for taxi domain.\n",
            "9. valListFile.txt: list of dialogues for validation.\n",
            "10. testListFile.txt: list of dialogues for testing.\n",
            "11. system_acts.json:\n",
            "  There are 6 domains ('Booking', 'Restaurant', 'Hotel', 'Attraction', 'Taxi', 'Train') and 1 dummy domain ('general').\n",
            "  A domain-dependent dialogue act is defined as a domain token followed by a domain-independent dialogue act, e.g. 'Hotel-inform' means it is an 'inform' act in the Hotel domain.\n",
            "  Dialogue acts which cannot take slots, e.g., 'good bye', are defined under the 'general' domain.\n",
            "  A slot-value pair defined as a list with two elements. The first element is slot token and the second one is its value.\n",
            "  If a dialogue act takes no slots, e.g., dialogue act 'offer booking' for an utterance 'would you like to take a reservation?', its slot-value pair is ['none', 'none']\n",
            "  There are four types of values:\n",
            "  1) If a slot takes a binary value, e.g., 'has Internet' or 'has park', the value is either 'yes' or 'no'.\n",
            "  2) If a slot is under the act 'request', e.g., 'request' about 'area', the value is expressed as '?'.\n",
            "  3) The value that appears in the utterance e.g., the name of a restaurant.\n",
            "  4) If for some reason the turn does not have an annotation then it is labeled as \"No Annotation.\"\n",
            "12. ontology.json: Data-based ontology containing all the values for the different slots in the domains.\n",
            "13. slot_descriptions.json: A collection of human-written slot descriptions for each slot in the dataset. Each slot has at least two descriptions.\n",
            "14. tokenization.md: A description of the tokenization preprocessing we had to perform to maintain consistency between the dialogue act annotations of DSTC 8 Track 1 and the existing MultiWOZ 2.0 data. \n",
            "\n"
          ]
        }
      ],
      "source": [
        "with open('/content/drive/MyDrive/data/MultiWOZ_2.1/README') as file:\n",
        "    print(file.read())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24x4EEgtjr1i",
        "outputId": "82b75b5b-4200-4873-b60a-8fed349bf184"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Person 1: am looking for a place to to stay that has cheap price range it should be in a type of hotel Person 2: Okay, do you have a specific area you want to stay in? Person 1: no, i just need to make sure it's cheap. oh, and i need parking Person 2: I found 1 cheap hotel for you that includes parking. Do you like me to book it? Person 1: Yes, please. 6 people 3 nights starting on tuesday. Person 2: I am sorry but I wasn't able to book that for you for Tuesday. Is there another day you would like to stay or perhaps a shorter stay? Person 1: how about only 2 nights. Person 2: Booking was successful.\n",
            "Reference number is : 7GAWK763. Anything else I can do for you? Person 1: No, that will be all. Good bye. Person 2: Thank you for using our services.\n"
          ]
        }
      ],
      "source": [
        "# the keys are the file names\n",
        "all_files = DIALOGUE_DB.keys()\n",
        "\n",
        "# initialize empty list\n",
        "untokenized_data = []\n",
        "\n",
        "# loop over all files\n",
        "for file in all_files:\n",
        "    # this is the graded function you coded\n",
        "    # returns a string delimited by Person 1 and Person 2\n",
        "    result = get_conversation(file, DIALOGUE_DB)\n",
        "\n",
        "    # append to the list\n",
        "    untokenized_data.append(result)\n",
        "\n",
        "# print the first element to check if it's the same as the one we got before\n",
        "print(untokenized_data[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lv9bAGF_jr4y",
        "outputId": "0b7f16fc-4f6d-4962-f875-7ab39003e118"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of conversations in the data set: 10438\n",
            "number of conversations in train set: 9917\n",
            "number of conversations in eval set: 521\n"
          ]
        }
      ],
      "source": [
        "# shuffle the list we generated above\n",
        "random.shuffle(untokenized_data)\n",
        "\n",
        "# define a cutoff (5% of the total length for this assignment)\n",
        "# convert to int because we will use it as a list index\n",
        "cut_off = int(len(untokenized_data) * .05)\n",
        "\n",
        "# slice the list. the last elements after the cut_off value will be the eval set. the rest is for training.\n",
        "train_data, eval_data = untokenized_data[:-cut_off], untokenized_data[-cut_off:]\n",
        "\n",
        "print(f'number of conversations in the data set: {len(untokenized_data)}')\n",
        "print(f'number of conversations in train set: {len(train_data)}')\n",
        "print(f'number of conversations in eval set: {len(eval_data)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-OnXo4KukCvd"
      },
      "outputs": [],
      "source": [
        "def stream(data):\n",
        "    # loop over the entire data\n",
        "    while True:\n",
        "        # get a random element\n",
        "        d = random.choice(data)\n",
        "\n",
        "        # yield a tuple pair of identical values\n",
        "        # (i.e. our inputs to the model will also be our targets during training)\n",
        "        yield (d, d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o2NH_Uj_kFMr"
      },
      "outputs": [],
      "source": [
        "# trax allows us to use combinators to generate our data pipeline\n",
        "data_pipeline = trax.data.Serial(\n",
        "    # randomize the stream\n",
        "    trax.data.Shuffle(),\n",
        "\n",
        "    # tokenize the data\n",
        "    trax.data.Tokenize(vocab_dir=VOCAB_DIR,\n",
        "                       vocab_file=VOCAB_FILE),\n",
        "\n",
        "    # filter too long sequences\n",
        "    trax.data.FilterByLength(2048),\n",
        "\n",
        "    # bucket by length\n",
        "    trax.data.BucketByLength(boundaries=[128, 256,  512, 1024],\n",
        "                             batch_sizes=[16,    8,    4,   2, 1]),\n",
        "\n",
        "    # add loss weights but do not add it to the padding tokens (i.e. 0)\n",
        "    trax.data.AddLossWeights(id_to_mask=0)\n",
        ")\n",
        "\n",
        "# apply the data pipeline to our train and eval sets\n",
        "train_stream = data_pipeline(stream(train_data))\n",
        "eval_stream = data_pipeline(stream(eval_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QcT3LvjrkHli",
        "outputId": "abae41de-9151-4f89-9465-a592a6f04c75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input shape:  (2, 1024)\n",
            " Person 1: Hello, I am looking for a guesthouse to stay in and I would like it to have free parking. Person 2: Does location or price range matter? Person 1: No but it would also need free wifi. Person 2: We have 21 guesthouses all over Cambridge, I can narrow that down further if you tell me what part of town you're interested in Person 1: I need the North part of town Person 2: How about the acorn guesthouse? Person 1: has it free parking? Person 2: Yes, the parking is free.  Person 1: Can you book it for me and get a reference number ? Person 2: Sure can!  What day are you checking in? Person 1: i would like to check in on monday at 5:30 Person 2: How many nights would you like to stay? Person 1: monday and I actually meant 17:30 and for 6 people.  Sorry, things are hectic and im a little scatterbrained. Person 2: But how many nights are you staying? Person 1: Just one please. Person 2: Booking was successful. Reference number is : 2JXK0SQ0. Anything else I can help with?  Person 1: I would like the contact number an car type.  Person 2: Sure. Where would you like to go to? Person 1: I need a taxi from the hotel to the restaurant please. Person 2: I can book that for you. What time would you like to leave and what restaurant? Person 1: Oh, I seem to have forgotten to ask you about the restaurant. Can you look up Taj Tandoori for me? Person 2: Sure, their number is 01223412299 Person 1: I'd like to book a table at Taj Tandoori for 6 people on Monday at 17:30 Person 2: I have you booked the reference number is JQ0WQ39K. What else can I do for you? Person 1: I'll also need a taxi. Person 2: Where are you going? Person 1: I need to commute between the guesthouse and the restaurant. Make sure it arrives by the booked time and send me the contact number and car type please. Person 2: You are set, look for a white tesla, contact number is \t07013573051, any other questions? Person 1: No, that's all for today. Thank you for all your help. Have a great day! Person 2: You're welcome! Have a great day.\n"
          ]
        }
      ],
      "source": [
        "# the stream generators will yield (input, target, weights). let's just grab the input for inspection\n",
        "inp, _, _ = next(train_stream)\n",
        "\n",
        "# print the shape. format is (batch size, token length)\n",
        "print(\"input shape: \", inp.shape)\n",
        "\n",
        "# detokenize the first element\n",
        "print(trax.data.detokenize(inp[0], vocab_dir=VOCAB_DIR, vocab_file=VOCAB_FILE))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exoz1Fq9kKB8"
      },
      "outputs": [],
      "source": [
        "# UNQ_C2\n",
        "# GRADED FUNCTION: reversible_layer_forward\n",
        "def reversible_layer_forward(x, f, g):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        x (np.array): an input vector or matrix\n",
        "        f (function): a function which operates on a vector/matrix\n",
        "        g (function): a function which operates on a vector/matrix\n",
        "    Returns:\n",
        "        y (np.array): an output vector or matrix whose form is determined by 'x', f and g\n",
        "    \"\"\"\n",
        "    # split the input vector into two (* along the last axis because it is the depth dimension)\n",
        "    x1, x2 = np.split(x, 2, axis=-1)\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "\n",
        "    # get y1 using equation 3\n",
        "    y1 = x1+f(x2)\n",
        "\n",
        "    # get y2 using equation 4\n",
        "    y2 = x2+g(y1)\n",
        "\n",
        "    # concatenate y1 and y2 along the depth dimension. be sure output is of type np.ndarray\n",
        "    y = np.concatenate([y1, y2], axis = -1)\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "    return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xyr4GEzBkNiJ"
      },
      "outputs": [],
      "source": [
        "# UNQ_C3\n",
        "# GRADED FUNCTION: reversible_layer_reverse\n",
        "def reversible_layer_reverse(y, f, g):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        y (np.array): an input vector or matrix\n",
        "        f (function): a function which operates on a vector/matrix of the form of 'y'\n",
        "        g (function): a function which operates on a vector/matrix of the form of 'y'\n",
        "    Returns:\n",
        "        y (np.array): an output vector or matrix whose form is determined by 'y', f and g\n",
        "    \"\"\"\n",
        "\n",
        "    # split the input vector into two (* along the last axis because it is the depth dimension)\n",
        "    y1, y2 = np.split(y, 2, axis=-1)\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "\n",
        "    # compute x2 using equation 5\n",
        "    x2 = y2 - g(y1)\n",
        "\n",
        "    # compute x1 using equation 6\n",
        "    x1 = y1-f(x2)\n",
        "\n",
        "    # concatenate x1 and x2 along the depth dimension\n",
        "    x = np.concatenate([x1, x2], axis = -1)\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJw4Z73ykQI1"
      },
      "outputs": [],
      "source": [
        "random_seed = 27686\n",
        "rng = trax.fastmath.random.get_prng(random_seed)\n",
        "f = lambda x: x + trax.fastmath.random.uniform(key=rng, shape=x.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ifIGpP2qkajg"
      },
      "outputs": [],
      "source": [
        "# UNQ_C4\n",
        "# GRADED FUNCTION\n",
        "def ReformerLM(vocab_size=33000, n_layers=2, mode='train', attention_type=tl.SelfAttention):\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "    # initialize an instance of Trax's ReformerLM class\n",
        "    model = tl.Serial(\n",
        "                trax.models.reformer.ReformerLM(\n",
        "                # set vocab size\n",
        "                vocab_size=vocab_size,\n",
        "                # set number of layers\n",
        "                n_layers=n_layers,\n",
        "                # set mode\n",
        "                mode=mode,\n",
        "                # set attention type\n",
        "                attention_type=attention_type\n",
        "            )\n",
        "            , tl.LogSoftmax()\n",
        "        )\n",
        "    ### END CODE HERE ###\n",
        "    return model # tl.Serial(model, tl.LogSoftmax(),)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFxprUiVke2X",
        "outputId": "31d0d2be-8ceb-46cf-c07b-ccb278527643"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Serial[\n",
            "  Serial[\n",
            "    Serial[\n",
            "      ShiftRight(1)\n",
            "    ]\n",
            "    Embedding_train_512\n",
            "    Dropout\n",
            "    Serial[\n",
            "      PositionalEncoding\n",
            "    ]\n",
            "    Dup_out2\n",
            "    ReversibleSerial_in2_out2[\n",
            "      ReversibleHalfResidualDecoderAttn_in2_out2[\n",
            "        Serial[\n",
            "          LayerNorm\n",
            "        ]\n",
            "        SelfAttention\n",
            "      ]\n",
            "      ReversibleSwap_in2_out2\n",
            "      ReversibleHalfResidualDecoderFF_in2_out2[\n",
            "        Serial[\n",
            "          LayerNorm\n",
            "          Dense_2048\n",
            "          Dropout\n",
            "          Serial[\n",
            "            FastGelu\n",
            "          ]\n",
            "          Dense_512\n",
            "          Dropout\n",
            "        ]\n",
            "      ]\n",
            "      ReversibleSwap_in2_out2\n",
            "      ReversibleHalfResidualDecoderAttn_in2_out2[\n",
            "        Serial[\n",
            "          LayerNorm\n",
            "        ]\n",
            "        SelfAttention\n",
            "      ]\n",
            "      ReversibleSwap_in2_out2\n",
            "      ReversibleHalfResidualDecoderFF_in2_out2[\n",
            "        Serial[\n",
            "          LayerNorm\n",
            "          Dense_2048\n",
            "          Dropout\n",
            "          Serial[\n",
            "            FastGelu\n",
            "          ]\n",
            "          Dense_512\n",
            "          Dropout\n",
            "        ]\n",
            "      ]\n",
            "      ReversibleSwap_in2_out2\n",
            "    ]\n",
            "    Concatenate_in2\n",
            "    LayerNorm\n",
            "    Dropout\n",
            "    Serial[\n",
            "      Dense_train\n",
            "    ]\n",
            "  ]\n",
            "  LogSoftmax\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "# display the model\n",
        "temp_model = ReformerLM('train')\n",
        "print(str(temp_model))\n",
        "\n",
        "# free memory\n",
        "#del temp_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NbZNb6Ttkkgc"
      },
      "outputs": [],
      "source": [
        "# UNQ_C5\n",
        "# GRADED FUNCTION: train_model\n",
        "def training_loop(ReformerLM, train_gen, eval_gen, output_dir = \"./model/\"):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        ReformerLM:  the Reformer language model you are building\n",
        "        train_gen (generator): train data generator.\n",
        "        eval_gen (generator): Validation generator.\n",
        "        output_dir (string): Path to save the model output. Defaults to './model/'.\n",
        "\n",
        "    Returns:\n",
        "        trax.supervised.training.Loop: Training loop for the model.\n",
        "    \"\"\"\n",
        "\n",
        "    # use the warmup_and_rsqrt_decay learning rate schedule\n",
        "    lr_schedule = trax.lr.warmup_and_rsqrt_decay(\n",
        "        n_warmup_steps=1000, max_value=0.01)\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "\n",
        "    # define the train task\n",
        "    train_task = training.TrainTask(\n",
        "        # labeled data\n",
        "        labeled_data=train_gen,\n",
        "        # loss layer\n",
        "        loss_layer=tl.CrossEntropyLoss(),\n",
        "        # optimizer\n",
        "        optimizer=trax.optimizers.Adam(0.01),\n",
        "        # lr_schedule\n",
        "        lr_schedule=lr_schedule,\n",
        "        # n_steps\n",
        "        n_steps_per_checkpoint=10\n",
        "    )\n",
        "\n",
        "    # define the eval task\n",
        "    eval_task = training.EvalTask(\n",
        "        # labeled data\n",
        "        labeled_data=eval_gen,\n",
        "        # metrics\n",
        "        metrics=[tl.CrossEntropyLoss(), tl.Accuracy()]\n",
        "    )\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "    loop = training.Loop(ReformerLM(mode='train'),\n",
        "                         train_task,\n",
        "                         eval_tasks=[eval_task],\n",
        "                         output_dir=output_dir)\n",
        "    return loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YkRF-6iGkptJ",
        "outputId": "b1a52a1b-be38-49e9-cf27-54a98f6fe4ab"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/jax/_src/xla_bridge.py:961: UserWarning: jax.host_count has been renamed to jax.process_count. This alias will eventually be removed; please update your code.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[<trax.supervised.training.TrainTask object at 0x79f9735c2dd0>]\n",
            "[<trax.supervised.training.EvalTask object at 0x79f9735c3880>]\n"
          ]
        }
      ],
      "source": [
        "# UNIT TEST COMMENT: Use the train task and eval task for grading train_model\n",
        "test_loop = training_loop(ReformerLM, train_stream, eval_stream)\n",
        "train_task = test_loop._tasks\n",
        "eval_task = test_loop._eval_tasks\n",
        "\n",
        "print(train_task)\n",
        "print(eval_task)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GbRNHOrakr-6",
        "outputId": "52597c02-da6f-4b4e-f836-e19271686d79"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/trax/layers/base.py:850: FutureWarning: GzipFile was opened for writing, but this will change in future Python releases.  Specify the mode argument for opening it for writing.\n",
            "  with gzip.GzipFile(fileobj=f, compresslevel=compresslevel) as gzipf:\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step      1: Total number of trainable weights: 58072296\n",
            "Step      1: Ran 1 train steps in 61.74 secs\n",
            "Step      1: train CrossEntropyLoss |  10.44404793\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/trax/supervised/training.py:1248: FutureWarning: GzipFile was opened for writing, but this will change in future Python releases.  Specify the mode argument for opening it for writing.\n",
            "  with gzip_lib.GzipFile(fileobj=f, compresslevel=2) as gzipf:\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step      1: eval  CrossEntropyLoss |  10.41295910\n",
            "Step      1: eval          Accuracy |  0.00000000\n",
            "\n",
            "Step     10: Ran 9 train steps in 182.23 secs\n",
            "Step     10: train CrossEntropyLoss |  10.21482754\n",
            "Step     10: eval  CrossEntropyLoss |  9.77753735\n",
            "Step     10: eval          Accuracy |  0.05338809\n"
          ]
        }
      ],
      "source": [
        "# we will now test your function\n",
        "!rm -f model/model.pkl.gz\n",
        "loop = training_loop(ReformerLM, train_stream, eval_stream)\n",
        "loop.run(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LLra4nb-kw7a"
      },
      "outputs": [],
      "source": [
        "# define the `predict_mem_len` and `predict_drop_len` of tl.SelfAttention\n",
        "def attention(*args, **kwargs):\n",
        "    # number of input positions to remember in a cache when doing fast inference.\n",
        "    kwargs['predict_mem_len'] = 120\n",
        "    # number of input elements to drop once the fast inference input cache fills up.\n",
        "    kwargs['predict_drop_len'] = 120\n",
        "    # return the attention layer with the parameters defined above\n",
        "    return tl.SelfAttention(*args, **kwargs)\n",
        "\n",
        "# define the model using the ReformerLM function you implemented earlier.\n",
        "model = ReformerLM(\n",
        "    vocab_size=33000,\n",
        "    n_layers=6,\n",
        "    mode='predict',\n",
        "    attention_type=attention,\n",
        ")\n",
        "\n",
        "# define an input signature so we can initialize our model. shape will be (1, 1) and the data type is int32.\n",
        "shape11 = trax.shapes.ShapeDtype((1, 1), dtype=np.int32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ranSLpiulRre"
      },
      "outputs": [],
      "source": [
        "def tokenize(sentence, vocab_file, vocab_dir):\n",
        "    return list(trax.data.tokenize(iter([sentence]), vocab_file=vocab_file, vocab_dir=vocab_dir))[0]\n",
        "\n",
        "def detokenize(tokens, vocab_file, vocab_dir):\n",
        "    return trax.data.detokenize(tokens, vocab_file=vocab_file, vocab_dir=vocab_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTNl5-eblVle"
      },
      "outputs": [],
      "source": [
        "# UNQ_C6\n",
        "# GRADED FUNCTION\n",
        "def ReformerLM_output_gen(ReformerLM, start_sentence, vocab_file, vocab_dir, temperature, tokenize=tokenize):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        ReformerLM:  the Reformer language model you just trained\n",
        "        start_sentence (string): starting sentence of the conversation\n",
        "        vocab_file (string): vocabulary filename\n",
        "        vocab_dir (string): directory of the vocabulary file\n",
        "        temperature (float): parameter for sampling ranging from 0.0 to 1.0.\n",
        "            0.0: same as argmax, always pick the most probable token\n",
        "            1.0: sampling from the distribution (can sometimes say random things)\n",
        "\n",
        "    Returns:\n",
        "        generator: yields the next symbol generated by the model\n",
        "    \"\"\"\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "\n",
        "    # Create input tokens using the the tokenize function\n",
        "    input_tokens = tokenize(start_sentence, vocab_file=vocab_file, vocab_dir=vocab_dir)\n",
        "\n",
        "    # Add batch dimension to array. Convert from (n,) to (x, n) where\n",
        "    # x is the batch size. Default is 1. (hint: you can use np.expand_dims() with axis=0)\n",
        "    input_tokens_with_batch = np.array(input_tokens)[None, :]\n",
        "\n",
        "    # call the autoregressive_sample_stream function from trax\n",
        "    output_gen = trax.supervised.decoding.autoregressive_sample_stream(\n",
        "        # model\n",
        "        ReformerLM,\n",
        "        # inputs will be the tokens with batch dimension\n",
        "        inputs=input_tokens_with_batch,\n",
        "        # temperature\n",
        "        temperature=temperature\n",
        "    )\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return output_gen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YTVzmywJlYD0"
      },
      "outputs": [],
      "source": [
        "shape11 = trax.shapes.ShapeDtype((1, 1), dtype=np.int32)\n",
        "\n",
        "def attention(*args, **kwargs):\n",
        "    kwargs['predict_mem_len'] = 120  # max length for predictions\n",
        "    kwargs['predict_drop_len'] = 120  # never drop old stuff\n",
        "    return tl.SelfAttention(*args, **kwargs)\n",
        "\n",
        "model = ReformerLM(\n",
        "    vocab_size=33000,\n",
        "    n_layers=6,\n",
        "    mode='predict',\n",
        "    attention_type=attention,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwWH0nyuqhDf",
        "outputId": "c545535f-2b8e-45b5-84cb-2541160ff2b1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/chatbot_model1.pkl/model.pkl.gz._tmp_: data\n"
          ]
        }
      ],
      "source": [
        "!file /content/drive/MyDrive/chatbot_model1.pkl/model.pkl.gz._tmp_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3hn5gF7qnPs"
      },
      "outputs": [],
      "source": [
        "!gzip /content/drive/MyDrive/chatbot_model1.pkl/model.pkl.gz._tmp_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kiRr8Nr8qyTl"
      },
      "outputs": [],
      "source": [
        "model.init_from_file('/content/drive/MyDrive/chatbot_model1.pkl/model.pkl.gz._tmp_.gz',\n",
        "                     weights_only=True, input_signature=shape11)\n",
        "\n",
        "STARTING_STATE = model.state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OSArQOrrlnn7"
      },
      "outputs": [],
      "source": [
        "def generate_dialogue(ReformerLM, model_state, start_sentence, vocab_file, vocab_dir, max_len, temperature):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        ReformerLM:  the Reformer language model you just trained\n",
        "        model_state (np.array): initial state of the model before decoding\n",
        "        start_sentence (string): starting sentence of the conversation\n",
        "        vocab_file (string): vocabulary filename\n",
        "        vocab_dir (string): directory of the vocabulary file\n",
        "        max_len (int): maximum number of tokens to generate\n",
        "        temperature (float): parameter for sampling ranging from 0.0 to 1.0.\n",
        "            0.0: same as argmax, always pick the most probable token\n",
        "            1.0: sampling from the distribution (can sometimes say random things)\n",
        "\n",
        "    Returns:\n",
        "        generator: yields the next symbol generated by the model\n",
        "    \"\"\"\n",
        "\n",
        "    # define the delimiters we used during training\n",
        "    delimiter_1 = 'Person 1: '\n",
        "    delimiter_2 = 'Person 2: '\n",
        "\n",
        "    # initialize detokenized output\n",
        "    sentence = ''\n",
        "\n",
        "    # token counter\n",
        "    counter = 0\n",
        "\n",
        "    # output tokens. we insert a ': ' for formatting\n",
        "    result = [tokenize(': ', vocab_file=vocab_file, vocab_dir=vocab_dir)]\n",
        "\n",
        "    # reset the model state when starting a new dialogue\n",
        "    ReformerLM.state = model_state\n",
        "\n",
        "    # calls the output generator implemented earlier\n",
        "    output = ReformerLM_output_gen(ReformerLM, start_sentence, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, temperature=temperature)\n",
        "\n",
        "    # print the starting sentence\n",
        "    print(start_sentence.split(delimiter_2)[0].strip())\n",
        "\n",
        "    # loop below yields the next tokens until max_len is reached. the if-elif is just for prettifying the output.\n",
        "    for o in output:\n",
        "\n",
        "        result.append(o)\n",
        "\n",
        "        sentence = detokenize(np.concatenate(result, axis=0), vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)\n",
        "\n",
        "        if sentence.endswith(delimiter_1):\n",
        "            sentence = sentence.split(delimiter_1)[0]\n",
        "            print(f'{delimiter_2}{sentence}')\n",
        "            sentence = ''\n",
        "            result.clear()\n",
        "\n",
        "        elif sentence.endswith(delimiter_2):\n",
        "            sentence = sentence.split(delimiter_2)[0]\n",
        "            print(f'{delimiter_1}{sentence}')\n",
        "            sentence = ''\n",
        "            result.clear()\n",
        "\n",
        "        counter += 1\n",
        "\n",
        "        if counter > max_len:\n",
        "            break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CjB3cqBslpy8",
        "outputId": "64cbbd15-98de-459d-c485-a9c0647666ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Person 1: Are there theatres in town?\n",
            "Person 2: : There are 4 theatres in town. Do you have a preference? \n",
            "Person 1: Not really, can you recommend one and give me the postcode and phone number? \n",
            "Person 2: Sure, the phone number is 01223576412 and the postcode is cb58as. \n",
            "Person 1: Thank you, I'm also looking for a train that leaves on Saturday after 17:15. \n"
          ]
        }
      ],
      "source": [
        "sample_sentence = ' Person 1: Are there theatres in town? Person 2: '\n",
        "generate_dialogue(ReformerLM=model, model_state=STARTING_STATE, start_sentence=sample_sentence, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, max_len=120, temperature=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UB0fxvWQlsxj"
      },
      "outputs": [],
      "source": [
        "sample_sentence = ' Person 1: Is there a hospital nearby? Person 2: '\n",
        "generate_dialogue(ReformerLM=model, model_state=STARTING_STATE, start_sentence=sample_sentence, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, max_len=120, temperature=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhXrHtKslvfx",
        "outputId": "2999c84e-7c70-4dce-9b94-33653a1fa08f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Person 1: I want to eat a pizza?\n",
            "Person 2: : I can help you with that. What is the price range you'd like to eat? \n",
            "Person 1: I would like to eat Italian. \n",
            "Person 2: I have 4 options for you. Do you have a price range in mind? \n",
            "Person 1: I would like to go to the south please. \n",
            "Person 1: I would recommend the Pizza Hut Cherry Hinton. It is expensive and in the south. Would you like to book a table? Person:  i i i want to go lovell i. i need the info \n"
          ]
        }
      ],
      "source": [
        "sample_sentence = ' Person 1: I want to eat a pizza? Person 2: '\n",
        "generate_dialogue(ReformerLM=model, model_state=STARTING_STATE, start_sentence=sample_sentence, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, max_len=120, temperature=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OHK-NUgwro_u"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
